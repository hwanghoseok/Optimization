{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization - QCQP, barrier method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.다음과 같은 QCQP 형태의 최적화 문제를 고려.\n",
    "##### $\\text{minimize}_x \\quad \\frac{1}{2}x^TQx + p^Tx $, \n",
    "##### $\\text{s.t} \\quad \\frac{1}{2}x^TQ_ix + p_{i}^{T}x \\leq 0  \\quad i = 1, \\dots , m$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 위의 부등제약 조건을 아래와 같이 지시함수 형태로 포함, 여전히 boundary를 가지고 잇고, 미분 불가능 하기 때문에 계산상 어려움 존재 \n",
    "##### $C = \\{x:\\frac{1}{2}x^TQ_ix + p_{i}^{T}x \\leq 0 \\quad i = 1, \\dots , m\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### $\\text{minimize}_x \\quad \\frac{1}{2}x^TQx + p^Tx + I_c(x)$ \n",
    "##### 해결) barrier function을 이용해 boundary 포함하지않고, 미분가능 하게해서 newton's method 적용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.logarithmic barrier function 구하기\n",
    "##### $h_i(x)=\\frac{1}{2}x^TQ_ix + p_{i}^{T}x 라고 하자.$ -> convex이고, 두번 미분가능\n",
    "##### $\\phi(x)= -\\sum_{i=1}^{m}log(-h_i(x))$ -> logarithmic barrier function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.barrier function을 적용 시킨 최적화 문제.\n",
    "##### $\\text{minimize}_x \\quad t(\\frac{1}{2}x^TQx + p^Tx) + \\phi(x)$ \n",
    "##### 이와 같이 근사해서 정의된 문제를 newton's method로 푸는 방법을 barrier method라고 함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.log barrier 계산.\n",
    "##### $h_i(x)=\\frac{1}{2}x^TQ_ix + p_{i}^{T}x$ \n",
    "##### $\\phi(x)= -\\sum_{i=1}^{m}log(-h_i(x))$ \n",
    "##### gradient: $\\bigtriangledown \\phi(x) = - \\sum_{i=1}^{m} \\frac{1}{h_i(x)}\\bigtriangledown h_i(x)$\n",
    "##### Hessian: $\\bigtriangledown^2 \\phi(x) = \\sum_{i=1}^{m} \\frac{1}{h_i(x)^2}\\bigtriangledown h_i(x)\\bigtriangledown h_i(x)^T - \\sum_{i=1}^{m} \\frac{1}{h_i(x)}\\bigtriangledown^2 h_i(x) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 \n",
      " x :\n",
      " [[-0.28777534]\n",
      " [-0.13002857]\n",
      " [-0.01936696]] \n",
      "obj : [[-0.24623822]]\n",
      "Iteration 1 \n",
      " x :\n",
      " [[-0.05790141]\n",
      " [-0.53790817]\n",
      " [-0.14390974]] \n",
      "obj : [[-0.47451565]]\n",
      "Iteration 2 \n",
      " x :\n",
      " [[ 0.47679513]\n",
      " [-1.35535807]\n",
      " [-0.38672035]] \n",
      "obj : [[-0.71071905]]\n",
      "Iteration 3 \n",
      " x :\n",
      " [[ 1.05098917]\n",
      " [-2.03842893]\n",
      " [-0.93986394]] \n",
      "obj : [[-0.47264719]]\n",
      "Iteration 4 \n",
      " x :\n",
      " [[ 1.13909119]\n",
      " [-2.10932333]\n",
      " [-1.10364447]] \n",
      "obj : [[-0.36016049]]\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function pfor.<locals>.f at 0x0000024589761D30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Iteration 5 \n",
      " x :\n",
      " [[ 1.13901824]\n",
      " [-2.10903491]\n",
      " [-1.11181366]] \n",
      "obj : [[-0.35464009]]\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function pfor.<locals>.f at 0x0000024589761DC0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Iteration 6 \n",
      " x :\n",
      " [[ 1.1389439 ]\n",
      " [-2.10898701]\n",
      " [-1.11181281]] \n",
      "obj : [[-0.35464155]]\n",
      "Iteration 7 \n",
      " x :\n",
      " [[ 1.13894389]\n",
      " [-2.10898701]\n",
      " [-1.11181281]] \n",
      "obj : [[-0.35464155]]\n",
      "Iteration 8 \n",
      " x :\n",
      " [[ 1.13894389]\n",
      " [-2.10898701]\n",
      " [-1.11181281]] \n",
      "obj : [[-0.35464155]]\n",
      "Iteration 9 \n",
      " x :\n",
      " [[ 1.13894389]\n",
      " [-2.10898701]\n",
      " [-1.11181281]] \n",
      "obj : [[-0.35464155]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "#data generation\n",
    "n = 3\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "Q1_ = np.random.random(size=(n,n))\n",
    "Q1 = tf.Variable(Q1_.T * Q1_)\n",
    "Q1 = tf.reshape(Q1, [1,3,3])\n",
    "Q2_ = np.random.random(size=(n,n))\n",
    "Q2 = tf.Variable(Q2_.T * Q2_)\n",
    "Q2 = tf.reshape(Q2, [1,3,3])\n",
    "Q3_ = np.random.random(size=(n,n))\n",
    "Q3 = tf.Variable(Q3_.T * Q3_)\n",
    "Q3 = tf.reshape(Q3, [1,3,3])\n",
    "Q0_ = np.random.random(size=(n,n))\n",
    "Q0 = tf.Variable(Q0_.T * Q0_)\n",
    "Q0 = tf.reshape(Q0, [1,3,3])\n",
    "\n",
    "Q = tf.concat([Q0, Q1, Q2, Q3], axis=0)\n",
    "\n",
    "c1 = tf.Variable(np.random.random(size=(n,1)))\n",
    "c1 = tf.reshape(c1, [1,3,1])\n",
    "c2 = tf.Variable(np.random.random(size=(n,1)))\n",
    "c2 = tf.reshape(c2, [1,3,1])\n",
    "c3 = tf.Variable(np.random.random(size=(n,1)))\n",
    "c3 = tf.reshape(c3, [1,3,1])\n",
    "c0 = tf.Variable(np.random.random(size=(n,1)))\n",
    "c0 = tf.reshape(c0, [1,3,1])\n",
    "\n",
    "c = tf.concat([c0, c1, c2, c3], axis=0)\n",
    "\n",
    "x = tf.Variable(-np.random.random(size=(n,1)))\n",
    "\n",
    "t = tf.constant(1)\n",
    "#%% objective function\n",
    "def h(Q,c,x):\n",
    "    return tf.matmul(tf.transpose(c), x) + tf.matmul(tf.matmul(tf.transpose(x), Q), x)/2\n",
    "\n",
    "def obj_fn(Q, c, x, t):\n",
    "    return t.numpy()*h(Q[0], c[0], x) -tf.reduce_sum(tf.math.log([-h(Q[1], c[1], x), -h(Q[2], c[2], x), -h(Q[3], c[3], x)]))\n",
    "#%% iteration\n",
    "for i in range(10):\n",
    "    print(\"Iteration\", i, \"\\n x :\\n\", x.numpy(), \"\\nobj :\", h(Q[0], c[0], x).numpy())\n",
    "    with tf.GradientTape(persistent=True) as tape2:\n",
    "        with tf.GradientTape(persistent=True) as tape1:\n",
    "            tape1.watch(x)\n",
    "            loss = obj_fn(Q, c, x, t)\n",
    "        grads = tape1.gradient(loss, x)\n",
    "    hessian = tf.reshape(tape2.jacobian(grads, x), shape=(3,3))\n",
    "    x = tf.Variable(x - tf.matmul(tf.linalg.inv(hessian), grads)) # x 업데이트\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3ddc9e21434b9ecfc9606e2895f7b76f8a1b37fd62762479223a6dfb4d4d2023"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('pyob': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
